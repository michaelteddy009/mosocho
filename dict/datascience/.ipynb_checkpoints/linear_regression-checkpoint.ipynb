{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#load the data\n",
    "data = pd.read_csv('datasets/uni_admission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  <u> (1) Linear Regression </u>\n",
    "First we will create a multivariable model, then check it's accuracy. For this we will use train_test_split to do split our data into training and testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The intercept/bias is -1.8004326417532597\n",
      "The coefficient(s) are [0.00544634 0.00583592 0.01426344 0.03586462]\n",
      "\n",
      "\n",
      "Mean Absolute Error: 0.0569147221978512\n",
      "Mean Squared Error: 0.00572592647322866\n",
      "Root Mean Squared Error: 0.07566985181185873\n"
     ]
    }
   ],
   "source": [
    "# We will use 4 independent variables for this\n",
    "X = data[['GRE', 'TOEFL', 'SOP', 'LOR']].values\n",
    "y = data['admit_chance'].values\n",
    "\n",
    "# Train using 80% of the data.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# find optimal coefficients and intercept\n",
    "regressor = LinearRegression()  \n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "#display the intercept / bias\n",
    "print(f\"The intercept/bias is {regressor.intercept_}\")\n",
    "\n",
    "#display the coefficient\n",
    "print(f\"The coefficient(s) are {regressor.coef_}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "# assess the accuraccy of the model\n",
    "y_predict = regressor.predict(X_test)\n",
    "\n",
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_predict))  \n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_predict))  \n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_predict)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> to access how good our model is we need to make predictions from our test dataset that we splitted then compare the result with the actuals to validate our desired accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual</th>\n",
       "      <th>Predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100.00000</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.70250</td>\n",
       "      <td>0.702903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.13282</td>\n",
       "      <td>0.122877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.39000</td>\n",
       "      <td>0.466832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.60000</td>\n",
       "      <td>0.611151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.70500</td>\n",
       "      <td>0.705319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.80000</td>\n",
       "      <td>0.800434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.96000</td>\n",
       "      <td>0.955068</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Actual   Predicted\n",
       "count  100.00000  100.000000\n",
       "mean     0.70250    0.702903\n",
       "std      0.13282    0.122877\n",
       "min      0.39000    0.466832\n",
       "25%      0.60000    0.611151\n",
       "50%      0.70500    0.705319\n",
       "75%      0.80000    0.800434\n",
       "max      0.96000    0.955068"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we make a comparison frame to view how we faired in making predictions\n",
    "\n",
    "comparison_frame = pd.DataFrame({'Actual': y_test.flatten(), 'Predicted': y_predict.flatten()})\n",
    "\n",
    "comparison_frame.describe()\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Now that we have made comparison of our prediction and the actual, we have an option of making a plot to visualize the line of best fit. We can make this plot with the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"plt.scatter(X_test, y_test, color='black')\\nplt.plot(X_test, y_predict, color='red')\\nplt.show()\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#if X_test was a just a column then we would have a plot using the code below.\n",
    "'''plt.scatter(X_test, y_test, color='black')\n",
    "plt.plot(X_test, y_predict, color='red')\n",
    "plt.show()'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> While visualizations are helpful in assessing our line of best fit. sklearn comes with crucial metrics for linear regression. These metrics are as follows \n",
    "* mean absolute error\n",
    "* Mean squared error\n",
    "* Root mean squared error\n",
    "\n",
    "> We will use these metrics to compare models to each other especially (RMSE). The lower the value the better. These values are relative to our dependent variable, i.e our dependent variable ranges from 0.39 to 0.96, with a mean of 0.7 and standard deviation of 0.13. This makes an RMSE of 0.085 somewhat acceptable. If our data ranged from 0 to 10, an RMSE of 0.085 would be incredible!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <u> Linear Regression Tip - Detecting multicollinearity </u>\n",
    "First we try to understand how the independent variables are related to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GRE</th>\n",
       "      <th>TOEFL</th>\n",
       "      <th>uni_rating</th>\n",
       "      <th>SOP</th>\n",
       "      <th>LOR</th>\n",
       "      <th>CGPA</th>\n",
       "      <th>publications</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>GRE</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.827200</td>\n",
       "      <td>0.635376</td>\n",
       "      <td>0.613498</td>\n",
       "      <td>0.524679</td>\n",
       "      <td>0.825878</td>\n",
       "      <td>0.563398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TOEFL</th>\n",
       "      <td>0.827200</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.649799</td>\n",
       "      <td>0.644410</td>\n",
       "      <td>0.541563</td>\n",
       "      <td>0.810574</td>\n",
       "      <td>0.467012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>uni_rating</th>\n",
       "      <td>0.635376</td>\n",
       "      <td>0.649799</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.728024</td>\n",
       "      <td>0.608651</td>\n",
       "      <td>0.705254</td>\n",
       "      <td>0.427047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SOP</th>\n",
       "      <td>0.613498</td>\n",
       "      <td>0.644410</td>\n",
       "      <td>0.728024</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.663707</td>\n",
       "      <td>0.712154</td>\n",
       "      <td>0.408116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LOR</th>\n",
       "      <td>0.524679</td>\n",
       "      <td>0.541563</td>\n",
       "      <td>0.608651</td>\n",
       "      <td>0.663707</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.637469</td>\n",
       "      <td>0.372526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CGPA</th>\n",
       "      <td>0.825878</td>\n",
       "      <td>0.810574</td>\n",
       "      <td>0.705254</td>\n",
       "      <td>0.712154</td>\n",
       "      <td>0.637469</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.501311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>publications</th>\n",
       "      <td>0.563398</td>\n",
       "      <td>0.467012</td>\n",
       "      <td>0.427047</td>\n",
       "      <td>0.408116</td>\n",
       "      <td>0.372526</td>\n",
       "      <td>0.501311</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   GRE     TOEFL  uni_rating       SOP       LOR      CGPA  \\\n",
       "GRE           1.000000  0.827200    0.635376  0.613498  0.524679  0.825878   \n",
       "TOEFL         0.827200  1.000000    0.649799  0.644410  0.541563  0.810574   \n",
       "uni_rating    0.635376  0.649799    1.000000  0.728024  0.608651  0.705254   \n",
       "SOP           0.613498  0.644410    0.728024  1.000000  0.663707  0.712154   \n",
       "LOR           0.524679  0.541563    0.608651  0.663707  1.000000  0.637469   \n",
       "CGPA          0.825878  0.810574    0.705254  0.712154  0.637469  1.000000   \n",
       "publications  0.563398  0.467012    0.427047  0.408116  0.372526  0.501311   \n",
       "\n",
       "              publications  \n",
       "GRE               0.563398  \n",
       "TOEFL             0.467012  \n",
       "uni_rating        0.427047  \n",
       "SOP               0.408116  \n",
       "LOR               0.372526  \n",
       "CGPA              0.501311  \n",
       "publications      1.000000  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove the serial and admit chance columns, we want to focus on our independent variables only.\n",
    "independent_only = data.drop(columns=['Serial No.', 'admit_chance'])\n",
    "\n",
    "# Let's display the correlations between the variables\n",
    "correlations = independent_only.corr()\n",
    "\n",
    "correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tabe above shows us how each variable relates to another. The coefficient of 1 across the diagonal makes sense, as a variable is perfectly correlated to itself. Let's use these correlations to compute the VIF score for each variable. This will require a little bit of linear algebra, but the approach is straightforward: we create a new dataframe with the *inverse* of the matrix above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GRE</th>\n",
       "      <th>TOEFL</th>\n",
       "      <th>uni_rating</th>\n",
       "      <th>SOP</th>\n",
       "      <th>LOR</th>\n",
       "      <th>CGPA</th>\n",
       "      <th>publications</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>GRE</th>\n",
       "      <td>4.464249</td>\n",
       "      <td>-1.919309</td>\n",
       "      <td>-0.167441</td>\n",
       "      <td>0.115539</td>\n",
       "      <td>0.163716</td>\n",
       "      <td>-1.829666</td>\n",
       "      <td>-0.738214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TOEFL</th>\n",
       "      <td>-1.919309</td>\n",
       "      <td>3.904213</td>\n",
       "      <td>-0.280590</td>\n",
       "      <td>-0.320530</td>\n",
       "      <td>0.008925</td>\n",
       "      <td>-1.216918</td>\n",
       "      <td>0.115389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>uni_rating</th>\n",
       "      <td>-0.167441</td>\n",
       "      <td>-0.280590</td>\n",
       "      <td>2.621036</td>\n",
       "      <td>-1.003439</td>\n",
       "      <td>-0.326820</td>\n",
       "      <td>-0.504916</td>\n",
       "      <td>-0.109544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SOP</th>\n",
       "      <td>0.115539</td>\n",
       "      <td>-0.320530</td>\n",
       "      <td>-1.003439</td>\n",
       "      <td>2.835210</td>\n",
       "      <td>-0.715324</td>\n",
       "      <td>-0.670228</td>\n",
       "      <td>-0.041512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LOR</th>\n",
       "      <td>0.163716</td>\n",
       "      <td>0.008925</td>\n",
       "      <td>-0.326820</td>\n",
       "      <td>-0.715324</td>\n",
       "      <td>2.033555</td>\n",
       "      <td>-0.650578</td>\n",
       "      <td>-0.096312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CGPA</th>\n",
       "      <td>-1.829666</td>\n",
       "      <td>-1.216918</td>\n",
       "      <td>-0.504916</td>\n",
       "      <td>-0.670228</td>\n",
       "      <td>-0.650578</td>\n",
       "      <td>4.777992</td>\n",
       "      <td>-0.064604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>publications</th>\n",
       "      <td>-0.738214</td>\n",
       "      <td>0.115389</td>\n",
       "      <td>-0.109544</td>\n",
       "      <td>-0.041512</td>\n",
       "      <td>-0.096312</td>\n",
       "      <td>-0.064604</td>\n",
       "      <td>1.494008</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   GRE     TOEFL  uni_rating       SOP       LOR      CGPA  \\\n",
       "GRE           4.464249 -1.919309   -0.167441  0.115539  0.163716 -1.829666   \n",
       "TOEFL        -1.919309  3.904213   -0.280590 -0.320530  0.008925 -1.216918   \n",
       "uni_rating   -0.167441 -0.280590    2.621036 -1.003439 -0.326820 -0.504916   \n",
       "SOP           0.115539 -0.320530   -1.003439  2.835210 -0.715324 -0.670228   \n",
       "LOR           0.163716  0.008925   -0.326820 -0.715324  2.033555 -0.650578   \n",
       "CGPA         -1.829666 -1.216918   -0.504916 -0.670228 -0.650578  4.777992   \n",
       "publications -0.738214  0.115389   -0.109544 -0.041512 -0.096312 -0.064604   \n",
       "\n",
       "              publications  \n",
       "GRE              -0.738214  \n",
       "TOEFL             0.115389  \n",
       "uni_rating       -0.109544  \n",
       "SOP              -0.041512  \n",
       "LOR              -0.096312  \n",
       "CGPA             -0.064604  \n",
       "publications      1.494008  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(np.linalg.inv(correlations.values), index = correlations.index, columns=correlations.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpreting the table is straightforward: The VIF score for each variable is found alongside the downwards sloping diagonal. GRE has a score of 4.46, TOEFL has a score of 3.9, uni_rating a score of 2.62, etc.\n",
    "\n",
    "CGPA has a value nearing 5, let's see how the VIF scores improve if we remove it from our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GRE</th>\n",
       "      <th>TOEFL</th>\n",
       "      <th>uni_rating</th>\n",
       "      <th>SOP</th>\n",
       "      <th>LOR</th>\n",
       "      <th>publications</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>GRE</th>\n",
       "      <td>3.763604</td>\n",
       "      <td>-2.385311</td>\n",
       "      <td>-0.360792</td>\n",
       "      <td>-0.141115</td>\n",
       "      <td>-0.085414</td>\n",
       "      <td>-0.762953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TOEFL</th>\n",
       "      <td>-2.385311</td>\n",
       "      <td>3.594274</td>\n",
       "      <td>-0.409188</td>\n",
       "      <td>-0.491232</td>\n",
       "      <td>-0.156772</td>\n",
       "      <td>0.098934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>uni_rating</th>\n",
       "      <td>-0.360792</td>\n",
       "      <td>-0.409188</td>\n",
       "      <td>2.567679</td>\n",
       "      <td>-1.074266</td>\n",
       "      <td>-0.395570</td>\n",
       "      <td>-0.116371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SOP</th>\n",
       "      <td>-0.141115</td>\n",
       "      <td>-0.491232</td>\n",
       "      <td>-1.074266</td>\n",
       "      <td>2.741195</td>\n",
       "      <td>-0.806583</td>\n",
       "      <td>-0.050574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LOR</th>\n",
       "      <td>-0.085414</td>\n",
       "      <td>-0.156772</td>\n",
       "      <td>-0.395570</td>\n",
       "      <td>-0.806583</td>\n",
       "      <td>1.944971</td>\n",
       "      <td>-0.105109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>publications</th>\n",
       "      <td>-0.762953</td>\n",
       "      <td>0.098934</td>\n",
       "      <td>-0.116371</td>\n",
       "      <td>-0.050574</td>\n",
       "      <td>-0.105109</td>\n",
       "      <td>1.493134</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   GRE     TOEFL  uni_rating       SOP       LOR  publications\n",
       "GRE           3.763604 -2.385311   -0.360792 -0.141115 -0.085414     -0.762953\n",
       "TOEFL        -2.385311  3.594274   -0.409188 -0.491232 -0.156772      0.098934\n",
       "uni_rating   -0.360792 -0.409188    2.567679 -1.074266 -0.395570     -0.116371\n",
       "SOP          -0.141115 -0.491232   -1.074266  2.741195 -0.806583     -0.050574\n",
       "LOR          -0.085414 -0.156772   -0.395570 -0.806583  1.944971     -0.105109\n",
       "publications -0.762953  0.098934   -0.116371 -0.050574 -0.105109      1.493134"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "revised = independent_only.drop(columns=['CGPA'])\n",
    "\n",
    "correlations = revised.corr()\n",
    "pd.DataFrame(np.linalg.inv(correlations.values), index = correlations.index, columns=correlations.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All scores dropped, but the GRE's in particular did quite a bit, indicating that GRE and CGPA were colinear. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <u> Linear Regression Tip - Residual Plots and heteroskedasticity testing </u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by creating a model based on our revised set of independent variables above, then displaying the residual plot for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0017516644688071615"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "X = revised.values\n",
    "y = data['admit_chance'].values\n",
    "\n",
    "X_train, X_test, admit_train, admit_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(X_train, admit_train)\n",
    "\n",
    "# This is our prediction for admission based on our model\n",
    "admit_predict = regressor.predict(X_test)\n",
    "\n",
    "# We now create the residual by substracting the test value from the predicted \n",
    "# value for each row in our dataset\n",
    "\n",
    "residuals = np.subtract(admit_predict, admit_test)\n",
    "\n",
    "# Let's describe our residual:\n",
    "pd.DataFrame(residuals).describe()\n",
    "\n",
    "residuals.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we interpret the description above, let's recall what we are trying to predict: The percentage chance of admission to university. This means values between 0 and 1. \n",
    "\n",
    "Our min and max for the residual are fairly high: they suggest we've been up to 26% off target. It's important for us to plot this first: Is this a common occurence, or a few outliers?\n",
    "\n",
    "Our mean on the other hand is close to 0, indicating that we tend to be fairly correct, although slightly over estimating chances by, on average, 0.17%\n",
    "\n",
    "Let's show the residual plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(admit_predict, residuals, color='black')\n",
    "plt.ylabel('residual')\n",
    "plt.xlabel('fitted values')\n",
    "plt.axhline(y= residuals.mean(), color='red', linewidth=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This does not look too bad: our residuals are centered around a mean that is very close to 0, and there are no glaringly obvious patterns. Let's be thorough though, and perform a heteroskedasticity test.\n",
    "\n",
    "For this we will use [bartlett's test](https://www.itl.nist.gov/div898/handbook/eda/section3/eda357.htm). The test establishes as a null hypothesis that the variance is equal for all our datapoints,and the new hypothesis that the variance is different for at least one pair of datapoints.\n",
    "\n",
    "NB: Barlett's test is used to test if k samples have equal variances. Equal variances across samples is called homogeneity of  variances. Some statistical tests, for example the analysis of variance, assume that variances are equal across groups or samples. Barlett's test is sensitive to departures from normality. That is, if your samples come from non-normal distributions, then Barlett's test may simply be testing for non-normality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "185.4720823514965\n",
      "the variances are homogeneous!\n"
     ]
    }
   ],
   "source": [
    "import scipy as sp\n",
    "\n",
    "test_result, p_value = sp.stats.bartlett(admit_predict, residuals)\n",
    "\n",
    "# To interpret the results we must also compute a critical value of the chi squared distribution\n",
    "degree_of_freedom = len(admit_predict)-1\n",
    "probability = 1 - p_value\n",
    "\n",
    "critical_value = sp.stats.chi2.ppf(probability, degree_of_freedom)\n",
    "print(critical_value)\n",
    "\n",
    "# If the test_result is greater than the critical value, then we reject our null\n",
    "# hypothesis. This would mean that there are patterns to the variance of the data\n",
    "\n",
    "# Otherwise, we can identify no patterns, and we accept the null hypothesis that \n",
    "# the variance is homogeneous across our data\n",
    "\n",
    "if (test_result > critical_value):\n",
    "  print('the variances are unequal, and the model should be reassessed')\n",
    "else:\n",
    "  print('the variances are homogeneous!')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
