{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <u> Notes </u>\n",
    "-  Linear regression algorithms are used to predict/forecast values while logistic regression algorithms are used for classification tasks i.e. 1 / 0, Yes / No, True / False given a set of independent variables. Some practical examples of classification tasks include classifying whether an email is a spam or not, classifying whether a tumour is malignant or benign, classifying whether a website is fraudulent or not, etc.  logistic regression transforms its output using the logistic sigmoid function to return a probability value which can then be mapped to two or more discrete classes.\n",
    "\n",
    "-Logistic regression is applicable, if:\n",
    "\n",
    "    * We want to model the probabilities of a response variable as a function of some explanatory variables, e.g. \"success\" of admission as a function of gender.\n",
    "    * We want to perform descriptive discriminate analyses such as describing the differences between individuals in separate groups as a function of explanatory variables, e.g. student admitted and rejected as a function of gender.\n",
    "    * We want to predict probabilities that individuals fall into two categories of the binary response as a function of some explanatory variables, e.g. what is the probability that a student is admitted given she is a female.\n",
    "    * We want to classify individuals into two categories based on explanatory variables, e.g. classify new students into \"admitted\" or \"rejected\" group depending on their gender.\n",
    "    \n",
    "-**Logistic regression can be classified in the following ways:**\n",
    "\n",
    "***Binomial Logistic Regression:*** target variable can have only 2 possible types: “0” or “1” which may represent “win” vs “loss”, “pass” vs “fail”, “dead” vs “alive”, etc.\n",
    "\n",
    "***Multinomial Logistic Regression:*** target variable can have 3 or more possible types which are not ordered(i.e. types have no quantitative significance) like “disease A” vs “disease B” vs “disease C”.\n",
    "\n",
    "***Ordinal Logistic Regression:*** it deals with target variables with ordered categories. For example, a test score can be categorized as:“very poor”, “poor”, “good”, “very good”. Here, each category can be given a score like 0, 1, 2, 3.\n",
    "\n",
    "\n",
    "-**In order to improve the accuracy of our logistic regression model we can perform any of the following techniques:**\n",
    "\n",
    "***Explore more classifiers*** - Logistic Regression learns a linear decision surface that separates our classes. It could be possible that our 2 classes may not be linearly separable. In such a case we might need to look at other classifiers such as Support Vector Machines which are able to learn more complex decision boundaries. We can also start looking at Tree-Based classifiers such as Decision Trees which can learn rules from our data. We can think of them as a series of If-Else rules which the algorithm automatically learns from the data.\n",
    "\n",
    "***Optimize other scores*** - We can optimize on other metrics also such as Log Loss and F1-Score. The F1-Score could be useful, in case of class imbalance. This is a good guide that talks more about scoring.\n",
    "    \n",
    "***Class Imbalance*** - We can look for class imbalance in our data. Since we are working with admit/reject data, then the number of rejects would be significantly higher than the admits. Most classifiers in SkLearn including LogisticRegression have a class_weight parameter. Setting that to balanced might also work well in case of a class imbalance.Most machine learning algorithms work best when the number of samples in each class are about equal. This is because most algorithms are designed to maximize accuracy and reduce error. Some of the ways to deal with class imbalance is includ 1) change performance metrix to use confusion matrix, recall, precision or f1-score. 2) Change the algorithm, it's important to note that decision trees always perform better in comparison to logistic regression when it comes to imbalanced classes. 3) resampling techniques - oversample minority class. 4) Undersample minority class 5) Generate synthetic samples.\n",
    "\n",
    "***Feature Scaling and/or Normalization*** - We can check the scales of our gre and gpa features. They differ on 2 orders of magnitude. Therefore, our gre feature will end up dominating the others in a classifier like Logistic Regression. We can normalize all our features to the same scale before putting them in a machine learning model.\n",
    "\n",
    "***Hyperparameter Tuning*** - Grid Search / Random Search - We can improve our accuracy by performing a Grid Search to tune the hyperparameters of our model. As we will get to learn, for example in case of LogisticRegression, the parameter C is a hyperparameter. Also, we should avoid using the test data during grid search. Instead perform cross-validation. Use our test data only to report the final numbers for our final model. We note that GridSearch should be done for all models that we try because then only we will be able to tell what is the best we can get from each model. Scikit-Learn provides the GridSearchCV class for this.\n",
    "\n",
    "***Error Analysis*** - For each of our models, go back and look at the cases where they are failing. We might end up finding that some of our models work well on one part of the parameter space while others work better on other parts. If this is the case, then Ensemble Techniques such as VotingClassifier techniques often give the best results. Models that win Kaggle competitions are many times ensemble models.Ensemble models in machine learning combine the decisions from multiple models to improve the overal performance. They are a divide and conquer approach used to improve performance. Simple ensemble techniques include taking the mode of the results, taking the average of the results, taking weighted average of the results, for advanced techniques we can use either bagging (bootstrap aggregating) and boosting techniques.bagging decreases the variance error .boosting in general decreases the bias error and builds strong predictive models. Boosting has shown better predictive accuracy than bagging, but it also tends to over-fit the training data as well.Thus, parameter tuning becomes a crucial part of boosting algorithms to make them avoid overfitting. Advantages include diversification (more accurate prediction results), stable and more robust models and it can also be used to capture linear and non-linear reletionships in data. Disadvantages include reduction in model intepret-ability, computation and design time is high, it's an art really hard to master.\n",
    "\n",
    "***More Features***  - If all of the above fail, we can start looking for more features.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NB:**\n",
    "- Logistic regression doesn't have the concept of residual so it can't use least squares and it cant use R-squared instead, it uses the concept of maximum likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <u> Example </u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading our dataset\n",
    "import pandas as pd\n",
    "\n",
    "titanic = pd.read_csv('../datasets/titanic.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting our dataset\n",
    "#\n",
    "X = titanic.drop([\"Survived\", 'Unnamed: 0'],axis=1)\n",
    "y = titanic[\"Survived\"]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .3, random_state=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting our model\n",
    "# \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "LogReg = LogisticRegression()\n",
    "LogReg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using our model to make a prediction\n",
    "#\n",
    "y_pred = LogReg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[137,  27],\n",
       "       [ 34,  69]], dtype=int64)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluating the model\n",
    "#\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix = confusion_matrix(y_test, y_pred)\n",
    "confusion_matrix\n",
    "\n",
    "# The results from the confusion matrix tell us that 137 and 69 are the number of correct predictions. \n",
    "# 34 and 27 are the number of incorrect predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
