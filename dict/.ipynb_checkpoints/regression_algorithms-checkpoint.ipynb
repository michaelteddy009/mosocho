{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  <u> Linear Regression </u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "regressor = LinearRegression()  #we first create a regressor object\n",
    "\n",
    "regressor.fit(X_train, y_train) #we fit looks for optimal line also know as line of best fit\n",
    "\n",
    "print(regressor.intercept_) #displays the intercept/bias\n",
    "print(regressor.coef_) #displays the coefficient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> to access how good our model is we need to make predictions from our test dataset that we splitted then compare the result with the actuals to validate our desired accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make prediction from our test dataframe\n",
    "y_predict = regressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we make a comparison frame to view how we faired in making predictions\n",
    "\n",
    "comparison_frame = pd.DataFrame({'Actual': y_test.flatten(), 'Predicted': y_predict.flatten()})\n",
    "\n",
    "comparison_frame.describe()\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Now that we have made comparison of our prediction and the actual, we have an option of making a plot to visualize the line of best fit. We can make this plot with the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_test, y_test, color='black')\n",
    "plt.plot(X_test, y_predict, color='red', linewidth=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> While visualizations are helpful in assessing our line of best fit. sklearn comes with crucial metrics for linear regression. These metrics are as follows \n",
    "* mean absolute error\n",
    "* Mean squared error\n",
    "* Root mean squared error\n",
    "\n",
    "> We will use these metrics to compare models to each other especially (RMSE). The lower the value the better. These values are relative to our dependent variable, i.e our dependent variable ranges from 0.39 to 0.96, with a mean of 0.7 and standard deviation of 0.13. This makes an RMSE of 0.085 somewhat acceptable. If our data ranged from 0 to 10, an RMSE of 0.085 would be incredible!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "\n",
    "#mean absolute error\n",
    "print(f\"Mean absolute error : {metrics.mean_absolute_error(y_test, y_predict)}\")\n",
    "\n",
    "#mean squared error\n",
    "print(f\"Mean squared error : {metrics.mean_squared_error(y_test, y_predict)}\")\n",
    "\n",
    "#Root mean squared error\n",
    "print(f\"Root mean squared error : {np.sqrt(metrics.mean_squared_error(y_test, y_predict))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
