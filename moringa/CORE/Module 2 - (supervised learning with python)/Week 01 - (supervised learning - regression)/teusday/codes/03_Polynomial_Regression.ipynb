{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X6fnxjPMh87f"
   },
   "source": [
    "# <font color=\"green\">Polynomial Regression in Python</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cQrBa1eIxgNj"
   },
   "source": [
    "# <font color=\"green\">Example 1</font> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DONC8-1iyE9D"
   },
   "source": [
    "Let's look at a practical way of performing a polynomial regression.\n",
    "\n",
    "In this example we are going to predict the salary of a candidate given the experience the candidate has. The HR of Sanford and Sons Limited, is looking to hire for a senior postion in the company. As a policy, the company has a salary distribution dataset that they use to determine how much salary to give to their employees based on their experience. As a data scientist, you are tasked creating a model that will help them with making salary predictions given  years of experience. In this case, we want to predict the amount of salary a canditate with 6 and half years of experience should get. You are provided with following dataset.\n",
    "\n",
    "[Download Dataset](https://drive.google.com/file/d/153tLxmGZxDhVmh2o0wUP8UjFPZPmW0Ux/view?usp=sharing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mJfuckv17tQQ"
   },
   "source": [
    "**Import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hcEDgdZd72YS"
   },
   "outputs": [],
   "source": [
    "#Import libaries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Import LinearRegression method from sklearn linear_model library\n",
    "from sklearn.linear_model import LinearRegression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9xnxzHXf8ZTP"
   },
   "source": [
    "**Load data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FUXiOEgg8jPp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Position  Level   Salary\n",
      "0   Business Analyst      1    45000\n",
      "1  Junior Consultant      2    50000\n",
      "2  Senior Consultant      3    60000\n",
      "3            Manager      4    80000\n",
      "4    Country Manager      5   110000\n",
      "5     Region Manager      6   150000\n",
      "6            Partner      7   200000\n",
      "7     Senior Partner      8   300000\n",
      "8            C-level      9   500000\n",
      "9                CEO     10  1000000\n",
      "[[ 1]\n",
      " [ 2]\n",
      " [ 3]\n",
      " [ 4]\n",
      " [ 5]\n",
      " [ 6]\n",
      " [ 7]\n",
      " [ 8]\n",
      " [ 9]\n",
      " [10]]\n",
      "[  45000   50000   60000   80000  110000  150000  200000  300000  500000\n",
      " 1000000]\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "dataset = pd.read_csv('datasets/position_salaries.csv')\n",
    "\n",
    "print(dataset)\n",
    "\n",
    "#Get the 2 and the last column from the dataset \n",
    "X = dataset.iloc[:, 1:2].values\n",
    "y = dataset.iloc[:, 2].values\n",
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "miJnKMeV9CTU"
   },
   "source": [
    "**Analysis of the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eyc8b6Dq-FQC"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Since our dataset is pretty clean, let's start by visualizing our dataset by plotting a scatter plot\n",
    "\n",
    "plt.scatter(X, y, color='blue') \n",
    "plt.title('Salary')\n",
    "plt.xlabel('Position Level')\n",
    "plt.ylabel('Salary')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n3Nm7UEW_Zvz"
   },
   "source": [
    "**Observation**\n",
    "\n",
    "From the scatter plot, the data does not appear to be linear. So if were to fit a simple linear straight line, it would not cover most of the points. Which is why applying Linear regression on this dataset would not give us the correct value.\n",
    "\n",
    "The other alternative we are left with is applying polynomial regression.\n",
    "\n",
    "**Manual prediction**\n",
    "\n",
    "Before we dive into polynomial regression, let's do some manual prediction of own and then we can compare the results we get to the results we'll get after doing polynomial regression in a short while.\n",
    "\n",
    "From our problem statement, we need to predict the salary of a candidate with 6.5 years of experience. From our dataset, we already know the salary for  candidates with 6 and 7 years of experience which is Ksh 150,000 and Ksh 200,000 respectively.This means that the salary for a candidate with 6.5 years of experience lies between this two years.Therefore, all we need to do is get the mean of this two years, right?\n",
    "\n",
    "(150,000 + 200,000)/ 2 = KSh 175,000\n",
    "\n",
    "From this, we can conclude that the desired salary is 175,000\n",
    "\n",
    "Now let's do it using polynomial regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wz5qKJgkFcZB"
   },
   "source": [
    "**Polynomial Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eGVsFvrKKF9N"
   },
   "outputs": [],
   "source": [
    "# Split the dataset into train and test sets\n",
    "X_train, Y_train, X_test, Y_test = train_test_split(X,y, test_size = 0.2, random_state=0)\n",
    "\n",
    "\n",
    "# Fit Linear Regression model to the dataset(this is optional. We are doing this for the sole purpose of comparing the linear regression model to the polynomial regression model)\n",
    "reg_line = LinearRegression()\n",
    "reg_line.fit(X,y)\n",
    "\n",
    "# Visualize the Linear Regression results\n",
    "plt.scatter(X, y, color='blue') # You can put any color you like. This just helps us highlight the data points\n",
    "plt.plot(X, reg_line.predict(X), color='red')\n",
    "plt.title('Salary Prediction(Linear Regression)')\n",
    "plt.xlabel('Position Level')\n",
    "plt.ylabel('Salary')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Fit polynomial Regression to the dataset\n",
    "poly_reg = PolynomialFeatures(degree = 2) \n",
    "X_poly = poly_reg.fit_transform(X)\n",
    "\n",
    "\n",
    "pol_reg = LinearRegression()\n",
    "pol_reg.fit(X_poly, y)\n",
    "\n",
    "#Visualize the Polynomial Regression results\n",
    "plt.scatter(X, y, color='blue') # You can put any color you like. This just helps us highlight the data points\n",
    "plt.plot(X, pol_reg.predict(X_poly), color='red')\n",
    "plt.title('Salary Prediction (Polynomial Regression)')\n",
    "plt.xlabel('Position Level')\n",
    "plt.ylabel('Salary')\n",
    "plt.show()\n",
    "\n",
    "# pol_reg.predict(poly_reg.fit_transform([[5.5]]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dC1ZQqSaIZoZ"
   },
   "source": [
    "**Code Explanation**\n",
    " \n",
    "Remember earlier on we said that a polynomial equation is written in the form;\n",
    "\n",
    "y = b0+b1x+b2x^2.....bnx^n\n",
    "\n",
    "Therefore,everytime we add an  nth degree of polynomial x its considered as though we are adding a feature.\n",
    "\n",
    "So the function \n",
    "\n",
    "\n",
    "```\n",
    "PolynomialFeatures(degree = 2)\n",
    "```\n",
    "\n",
    "\n",
    "helps us to add the degree polynomial x.For example, in our code we have specified the degree to be 2 hence the equation would look like this\n",
    "\n",
    "y = b0+b1x+b2x^2\n",
    "\n",
    "\n",
    "On the next line, we fit and transform our X independent variable into a format that has the polynomial features\n",
    "\n",
    "Next, we apply our polynomial feature to linear regression. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kXwqH8BhW8OF"
   },
   "source": [
    "**Observation**\n",
    "\n",
    "Looking at the two graphs, we can clearly see that the polynomial graph has given us way better results than the simple linear one.\n",
    "\n",
    "Using the polynomial graph it becomes easier to draw meaningful conclusion from it. However, the curve is not passing through as many points as we'd want hence we can say that this graph is not fully optimized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-sKhhgoCZN5Q"
   },
   "source": [
    "**Optimization in Polynomial Regression**\n",
    "\n",
    "How do we choose an optimal model?\n",
    "\n",
    "For us to answer this question, we need to first understand this 3 very important terms\n",
    "\n",
    "\n",
    "1.   Bias\n",
    "2.   Variance\n",
    "\n",
    "\n",
    "**Bias**: This is the error we get due to the model's simplistic assumptions when fitting a dataset. In other words, a high bias translates to the model being unable to capture the pattern in a dataset. As a result, it leads to **under-fitting**. A perfect example is when we tried to use simple linear regression in our dataset.\n",
    "\n",
    "**Variance**: Unlike bias, this is the error we get due to trying to fit a complex model to our data. A high variance translates to the model passing through most of the data points, leading to **over-fitting**. This is not good because the model will fail to generalise on unseen data. This means that if we subject the model to another dataset it will fail to fit the pattern . \n",
    "\n",
    "Therefore, for us to choose an optimal model the model needs to have a low bias and a low varinace.That is to say the model should not capture too many data points and at the same time its should not capture very few data points. \n",
    "\n",
    "So now let's optimize our model\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p1IBBtLAg6RW"
   },
   "outputs": [],
   "source": [
    "# To improve on the model, all we need to do is to increase degree of the polynomial\n",
    "# Let's increase the degree to 3\n",
    "\n",
    "# Fit polynomial Regression to the dataset\n",
    "poly_reg = PolynomialFeatures(degree = 3) \n",
    "X_poly = poly_reg.fit_transform(X)\n",
    "\n",
    "\n",
    "pol_reg = LinearRegression()\n",
    "pol_reg.fit(X_poly, y)\n",
    "\n",
    "#Visualize the Polynomial Regression results\n",
    "plt.scatter(X, y, color='blue') # You can put any color you like. This just helps us highlight the data points\n",
    "plt.plot(X, pol_reg.predict(X_poly), color='red')\n",
    "plt.title('Salary Prediction (Polynomial Regression)')\n",
    "plt.xlabel('Position Level')\n",
    "plt.ylabel('Salary')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7jmdajSDiq1m"
   },
   "source": [
    "**Observation**\n",
    "\n",
    "When we increase the degree to 3, the curve captures slightly more data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cYNew_n7jL1d"
   },
   "outputs": [],
   "source": [
    "# Let's improve it the model further\n",
    "\n",
    "# Fit polynomial Regression to the dataset\n",
    "poly_reg = PolynomialFeatures(degree = 6) \n",
    "X_poly = poly_reg.fit_transform(X)\n",
    "\n",
    "\n",
    "pol_reg = LinearRegression()\n",
    "pol_reg.fit(X_poly, y)\n",
    "\n",
    "#Visualize the Polynomial Regression results\n",
    "plt.scatter(X, y, color='blue') # You can put any color you like. This just helps us highlight the data points\n",
    "plt.plot(X, pol_reg.predict(X_poly), color='red')\n",
    "plt.title('Salary Prediction (Polynomial Regression)')\n",
    "plt.xlabel('Position Level')\n",
    "plt.ylabel('Salary')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "18OUnCvNjcm-"
   },
   "source": [
    "**Observation**\n",
    "\n",
    "When we increase the degree to 6, the model is able to capture all of the data points. This is an example of over-fitting. This model is simply too perfect which is not good for us when we using an unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x77UTOG1kNN2"
   },
   "outputs": [],
   "source": [
    "# Now we know where our cap is, let's decrease the degrees slightly \n",
    "\n",
    "# Fit polynomial Regression to the dataset\n",
    "poly_reg_4 = PolynomialFeatures(degree = 4) \n",
    "X_poly = poly_reg_4.fit_transform(X)\n",
    "\n",
    "\n",
    "pol_reg = LinearRegression()\n",
    "pol_reg.fit(X_poly, y)\n",
    "\n",
    "#Visualize the Polynomial Regression results\n",
    "plt.scatter(X, y, color='blue') # You can put any color you like. This just helps us highlight the data points\n",
    "plt.plot(X, pol_reg.predict(X_poly), color='red')\n",
    "plt.title('Salary Prediction (Polynomial Regression)')\n",
    "plt.xlabel('Position Level')\n",
    "plt.ylabel('Salary')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OiQRbRSSk0EF"
   },
   "source": [
    "**Obsevartion**\n",
    "\n",
    "We can conclude that this is a pretty good model because it passes just about the right amoung of points and also the  distance from the curve to the points that it does not pass through is at a minimum. Hence, we can use this model to make our predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8IABpBSZuW1g"
   },
   "source": [
    "**Making Predictions**\n",
    "\n",
    "The final step is use the model we created to make the our desired prediction.\n",
    "\n",
    "To check how accurate polynomial regression is, we are going to predict the salary using both linear regression and polynomial regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kR-M0RXZweuX"
   },
   "outputs": [],
   "source": [
    "# Predict using linear regression\n",
    "lin_pred = reg_line.predict([[6.5]])\n",
    "print('Linear prediction: %d' %lin_pred)\n",
    "\n",
    "# Predict using Polynomial Regression\n",
    "poly_pred = pol_reg.predict(poly_reg_4.fit_transform([[6.5]]))\n",
    "print('Polynomial prediction: %d' %poly_pred)\n",
    "\n",
    "# We can also get the predictions as an array\n",
    "poly_arr_pred = pol_reg.predict(poly_reg_4.fit_transform(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7aKkBsgnFnNs"
   },
   "source": [
    "**Conclusion**\n",
    "\n",
    "The predicted salary using linear regression is Ksh 330,379. This is not accepteable because it falls out of the range Ksh 150,000 and 200,000.\n",
    "\n",
    "On the other hand,the predicted salary using polynomial regression is Ksh 158,862"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fv9VRI1n9pIb"
   },
   "source": [
    "# <font color=\"green\">Challenges</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gHJa-iFU91TH"
   },
   "outputs": [],
   "source": [
    "# Challenge 1\n",
    "# You are given a dataset that contains the prices and other attributes of diamonds. The dataset has 9 columns: Carat, color, cut, clarity, depth, table, price, x, y, z.\n",
    "# carat: Carat weight of the diamond\n",
    "# cut: Describes cut quality of the diamond. Quality in increasing order Fair, Good, Very Good, Premium, Ideal\n",
    "# color: Color of the diamond, with D being the best and J the worst\n",
    "# clarity: How obvious inclusions are within the diamond:(in order from best to worst, FL = flawless, I3= level 3 inclusions) FL,IF, VVS1, VVS2, VS1, VS2, SI1, SI2, I1, I2, I3\n",
    "# depth % :The height of a diamond, measured from the culet to the table, divided by its average girdle diameter\n",
    "# table %: The width of the diamond's table expressed as a percentage of its average diameter\n",
    "# price: The price of the diamond\n",
    "# x: length mm\n",
    "# y: width mm\n",
    "# z: depth mm\n",
    "# Your task is to predict the prices of diamonds given carat weight\n",
    "# Here is the dataset: https://drive.google.com/file/d/1lSgupNhVwXzR9aXbz2G0VwKQGEpx6pFl/view?usp=sharing\n",
    "\n",
    "Your code goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ezh0hqhRNSwE"
   },
   "outputs": [],
   "source": [
    "# Challenge 2\n",
    "# The dataset for this challenge represents the physicochemical tests of wine and the quality of wine as a result of these physicochemical tests. Here's a brief description of the columns\n",
    "# volatile acidity: The amount of acetic acid in wine, which at too high of levels can lead to an unpleasant, vinegar taste\n",
    "# citric acid: Found in small quantities, citric acid can add 'freshness' and flavor to wines\n",
    "# residual sugar: The amount of sugar remaining after fermentation stops, it's rare to find wines with less than 1 gram/liter and wines with greater than 45 grams/liter are considered sweet\n",
    "# chlorides: The amount of salt in the wine\n",
    "# free sulfur dioxide: The free form of SO2 exists in equilibrium between molecular SO2 (as a dissolved gas) and bisulfite ion; it prevents microbial growth and the oxidation of wine\n",
    "# total sulfur dioxide: Amount of free and bound forms of S02; in low concentrations, SO2 is mostly undetectable in wine, but at free SO2 concentrations over 50 ppm, SO2 becomes evident in the nose and taste of wine\n",
    "# density: The density of water is close to that of water depending on the percent alcohol and sugar content\n",
    "# pH: Describes how acidic or basic a wine is on a scale from 0 (very acidic) to 14 (very basic); most wines are between 3-4 on the pH scale\n",
    "# sulphates: A wine additive which can contribute to sulfur dioxide gas (S02) levels, wich acts as an antimicrobial and antioxidant\n",
    "# alcohol: The percent alcohol content of the wine\n",
    "# quality: Output variable (based on sensory data, score between 0 and 10)\n",
    "\n",
    "# Here is a link to the dataset: https://drive.google.com/open?id=1cctsp46KVzNKGAK0kLl3mY6wRTDlUBp4\n",
    "\n",
    "# Use polynomial regression to predict quality of wine using the physiochemical tests\n",
    "\n",
    "# Hint:Try to get how well your model has been fitted using RMSE without necessarily plotting a graph\n",
    "\n",
    "Your code goes here"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Polynomial Regression.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
