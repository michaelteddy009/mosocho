{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oxXI2w6xSCxt"
   },
   "source": [
    "# Example: Detecting multicollinearity\n",
    "\n",
    "We will once again use our university admission [dataset](https://drive.google.com/file/d/13HPgfc4HP9UP-gHM2lutJ7MXSt-dAL1d/view)\n",
    "\n",
    "First, let's understand how our independent variables are correlated to each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 254
    },
    "colab_type": "code",
    "id": "3myU4PcKSB9V",
    "outputId": "e9e4a86f-0d5c-4904-c849-4e5f974f8dc3"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GRE</th>\n",
       "      <th>TOEFL</th>\n",
       "      <th>uni_rating</th>\n",
       "      <th>SOP</th>\n",
       "      <th>LOR</th>\n",
       "      <th>CGPA</th>\n",
       "      <th>publications</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>GRE</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.827200</td>\n",
       "      <td>0.635376</td>\n",
       "      <td>0.613498</td>\n",
       "      <td>0.524679</td>\n",
       "      <td>0.825878</td>\n",
       "      <td>0.563398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TOEFL</th>\n",
       "      <td>0.827200</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.649799</td>\n",
       "      <td>0.644410</td>\n",
       "      <td>0.541563</td>\n",
       "      <td>0.810574</td>\n",
       "      <td>0.467012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>uni_rating</th>\n",
       "      <td>0.635376</td>\n",
       "      <td>0.649799</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.728024</td>\n",
       "      <td>0.608651</td>\n",
       "      <td>0.705254</td>\n",
       "      <td>0.427047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SOP</th>\n",
       "      <td>0.613498</td>\n",
       "      <td>0.644410</td>\n",
       "      <td>0.728024</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.663707</td>\n",
       "      <td>0.712154</td>\n",
       "      <td>0.408116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LOR</th>\n",
       "      <td>0.524679</td>\n",
       "      <td>0.541563</td>\n",
       "      <td>0.608651</td>\n",
       "      <td>0.663707</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.637469</td>\n",
       "      <td>0.372526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CGPA</th>\n",
       "      <td>0.825878</td>\n",
       "      <td>0.810574</td>\n",
       "      <td>0.705254</td>\n",
       "      <td>0.712154</td>\n",
       "      <td>0.637469</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.501311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>publications</th>\n",
       "      <td>0.563398</td>\n",
       "      <td>0.467012</td>\n",
       "      <td>0.427047</td>\n",
       "      <td>0.408116</td>\n",
       "      <td>0.372526</td>\n",
       "      <td>0.501311</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   GRE     TOEFL  uni_rating       SOP       LOR      CGPA  \\\n",
       "GRE           1.000000  0.827200    0.635376  0.613498  0.524679  0.825878   \n",
       "TOEFL         0.827200  1.000000    0.649799  0.644410  0.541563  0.810574   \n",
       "uni_rating    0.635376  0.649799    1.000000  0.728024  0.608651  0.705254   \n",
       "SOP           0.613498  0.644410    0.728024  1.000000  0.663707  0.712154   \n",
       "LOR           0.524679  0.541563    0.608651  0.663707  1.000000  0.637469   \n",
       "CGPA          0.825878  0.810574    0.705254  0.712154  0.637469  1.000000   \n",
       "publications  0.563398  0.467012    0.427047  0.408116  0.372526  0.501311   \n",
       "\n",
       "              publications  \n",
       "GRE               0.563398  \n",
       "TOEFL             0.467012  \n",
       "uni_rating        0.427047  \n",
       "SOP               0.408116  \n",
       "LOR               0.372526  \n",
       "CGPA              0.501311  \n",
       "publications      1.000000  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('datasets/uni_admission.csv')\n",
    "\n",
    "# Remove the serial and admit chance columns, we want to focus on our independent variables only.\n",
    "independent_only = data.drop(columns=['Serial No.', 'admit_chance'])\n",
    "\n",
    "# Let's display the correlations between the variables\n",
    "correlations = independent_only.corr()\n",
    "\n",
    "correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l3mXiNPXcMeF"
   },
   "source": [
    "The tabe above shows us how each variable relates to another. The coefficient of 1 across the diagonal makes sense, as a variable is perfectly correlated to itself. Let's use these correlations to compute the VIF score for each variable. This will require a little bit of linear algebra, but the approach is straightforward: we create a new dataframe with the *inverse* of the matrix above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 254
    },
    "colab_type": "code",
    "id": "Q_L2Glx4UJjW",
    "outputId": "d10406df-158c-485c-bec7-3e20b4f9724f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GRE</th>\n",
       "      <th>TOEFL</th>\n",
       "      <th>uni_rating</th>\n",
       "      <th>SOP</th>\n",
       "      <th>LOR</th>\n",
       "      <th>CGPA</th>\n",
       "      <th>publications</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>GRE</th>\n",
       "      <td>4.464249</td>\n",
       "      <td>-1.919309</td>\n",
       "      <td>-0.167441</td>\n",
       "      <td>0.115539</td>\n",
       "      <td>0.163716</td>\n",
       "      <td>-1.829666</td>\n",
       "      <td>-0.738214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TOEFL</th>\n",
       "      <td>-1.919309</td>\n",
       "      <td>3.904213</td>\n",
       "      <td>-0.280590</td>\n",
       "      <td>-0.320530</td>\n",
       "      <td>0.008925</td>\n",
       "      <td>-1.216918</td>\n",
       "      <td>0.115389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>uni_rating</th>\n",
       "      <td>-0.167441</td>\n",
       "      <td>-0.280590</td>\n",
       "      <td>2.621036</td>\n",
       "      <td>-1.003439</td>\n",
       "      <td>-0.326820</td>\n",
       "      <td>-0.504916</td>\n",
       "      <td>-0.109544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SOP</th>\n",
       "      <td>0.115539</td>\n",
       "      <td>-0.320530</td>\n",
       "      <td>-1.003439</td>\n",
       "      <td>2.835210</td>\n",
       "      <td>-0.715324</td>\n",
       "      <td>-0.670228</td>\n",
       "      <td>-0.041512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LOR</th>\n",
       "      <td>0.163716</td>\n",
       "      <td>0.008925</td>\n",
       "      <td>-0.326820</td>\n",
       "      <td>-0.715324</td>\n",
       "      <td>2.033555</td>\n",
       "      <td>-0.650578</td>\n",
       "      <td>-0.096312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CGPA</th>\n",
       "      <td>-1.829666</td>\n",
       "      <td>-1.216918</td>\n",
       "      <td>-0.504916</td>\n",
       "      <td>-0.670228</td>\n",
       "      <td>-0.650578</td>\n",
       "      <td>4.777992</td>\n",
       "      <td>-0.064604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>publications</th>\n",
       "      <td>-0.738214</td>\n",
       "      <td>0.115389</td>\n",
       "      <td>-0.109544</td>\n",
       "      <td>-0.041512</td>\n",
       "      <td>-0.096312</td>\n",
       "      <td>-0.064604</td>\n",
       "      <td>1.494008</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   GRE     TOEFL  uni_rating       SOP       LOR      CGPA  \\\n",
       "GRE           4.464249 -1.919309   -0.167441  0.115539  0.163716 -1.829666   \n",
       "TOEFL        -1.919309  3.904213   -0.280590 -0.320530  0.008925 -1.216918   \n",
       "uni_rating   -0.167441 -0.280590    2.621036 -1.003439 -0.326820 -0.504916   \n",
       "SOP           0.115539 -0.320530   -1.003439  2.835210 -0.715324 -0.670228   \n",
       "LOR           0.163716  0.008925   -0.326820 -0.715324  2.033555 -0.650578   \n",
       "CGPA         -1.829666 -1.216918   -0.504916 -0.670228 -0.650578  4.777992   \n",
       "publications -0.738214  0.115389   -0.109544 -0.041512 -0.096312 -0.064604   \n",
       "\n",
       "              publications  \n",
       "GRE              -0.738214  \n",
       "TOEFL             0.115389  \n",
       "uni_rating       -0.109544  \n",
       "SOP              -0.041512  \n",
       "LOR              -0.096312  \n",
       "CGPA             -0.064604  \n",
       "publications      1.494008  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(np.linalg.inv(correlations.values), index = correlations.index, columns=correlations.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jTBZHScKc5yh"
   },
   "source": [
    "Interpreting the table is straightforward: The VIF score for each variable is found alongside the downwards sloping diagonal. GRE has a score of 4.46, TOEFL has a score of 3.9, uni_rating a score of 2.62, etc.\n",
    "\n",
    "CGPA has a value nearing 5, let's see how the VIF scores improve if we remove it from our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 225
    },
    "colab_type": "code",
    "id": "8ODEkFCQaG94",
    "outputId": "f466f621-3d40-45cc-f2d5-4a7711251640"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GRE</th>\n",
       "      <th>TOEFL</th>\n",
       "      <th>uni_rating</th>\n",
       "      <th>SOP</th>\n",
       "      <th>LOR</th>\n",
       "      <th>publications</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>GRE</th>\n",
       "      <td>3.763604</td>\n",
       "      <td>-2.385311</td>\n",
       "      <td>-0.360792</td>\n",
       "      <td>-0.141115</td>\n",
       "      <td>-0.085414</td>\n",
       "      <td>-0.762953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TOEFL</th>\n",
       "      <td>-2.385311</td>\n",
       "      <td>3.594274</td>\n",
       "      <td>-0.409188</td>\n",
       "      <td>-0.491232</td>\n",
       "      <td>-0.156772</td>\n",
       "      <td>0.098934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>uni_rating</th>\n",
       "      <td>-0.360792</td>\n",
       "      <td>-0.409188</td>\n",
       "      <td>2.567679</td>\n",
       "      <td>-1.074266</td>\n",
       "      <td>-0.395570</td>\n",
       "      <td>-0.116371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SOP</th>\n",
       "      <td>-0.141115</td>\n",
       "      <td>-0.491232</td>\n",
       "      <td>-1.074266</td>\n",
       "      <td>2.741195</td>\n",
       "      <td>-0.806583</td>\n",
       "      <td>-0.050574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LOR</th>\n",
       "      <td>-0.085414</td>\n",
       "      <td>-0.156772</td>\n",
       "      <td>-0.395570</td>\n",
       "      <td>-0.806583</td>\n",
       "      <td>1.944971</td>\n",
       "      <td>-0.105109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>publications</th>\n",
       "      <td>-0.762953</td>\n",
       "      <td>0.098934</td>\n",
       "      <td>-0.116371</td>\n",
       "      <td>-0.050574</td>\n",
       "      <td>-0.105109</td>\n",
       "      <td>1.493134</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   GRE     TOEFL  uni_rating       SOP       LOR  publications\n",
       "GRE           3.763604 -2.385311   -0.360792 -0.141115 -0.085414     -0.762953\n",
       "TOEFL        -2.385311  3.594274   -0.409188 -0.491232 -0.156772      0.098934\n",
       "uni_rating   -0.360792 -0.409188    2.567679 -1.074266 -0.395570     -0.116371\n",
       "SOP          -0.141115 -0.491232   -1.074266  2.741195 -0.806583     -0.050574\n",
       "LOR          -0.085414 -0.156772   -0.395570 -0.806583  1.944971     -0.105109\n",
       "publications -0.762953  0.098934   -0.116371 -0.050574 -0.105109      1.493134"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "revised = independent_only.drop(columns=['CGPA'])\n",
    "\n",
    "correlations = revised.corr()\n",
    "pd.DataFrame(np.linalg.inv(correlations.values), index = correlations.index, columns=correlations.columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z95zlmBidiF5"
   },
   "source": [
    "All scores dropped, but the GRE's in particular did quite a bit, indicating that GRE and CGPA were colinear. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7DRwrZjZduD7"
   },
   "source": [
    "# Example 2: Residual plots and heteroskedasticity testing\n",
    "\n",
    "Let's start by creating a model based on our revised set of independent variables above, then displaying the residual plot for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "IHQOLs1Nh2bx",
    "outputId": "45772409-dfe6-43f8-d545-04500fb8d13a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0017516644688071348"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "X = revised.values\n",
    "y = data['admit_chance'].values\n",
    "\n",
    "X_train, X_test, admit_train, admit_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(X_train, admit_train)\n",
    "\n",
    "# This is our prediction for admission based on our model\n",
    "admit_predict = regressor.predict(X_test)\n",
    "\n",
    "# We now create the residual by substracting the test value from the predicted \n",
    "# value for each row in our dataset\n",
    "\n",
    "residuals = np.subtract(admit_predict, admit_test)\n",
    "\n",
    "# Let's describe our residual:\n",
    "pd.DataFrame(residuals).describe()\n",
    "\n",
    "residuals.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wevY_SU-j7gH"
   },
   "source": [
    "Before we interpret the description above, let's recall what we are trying to predict: The percentage chance of admission to university. This means values between 0 and 1. \n",
    "\n",
    "Our min and max for the residual are fairly high: they suggest we've been up to 26% off target. It's important for us to plot this first: Is this a common occurence, or a few outliers?\n",
    "\n",
    "Our mean on the other hand is close to 0, indicating that we tend to be fairly correct, although slightly over estimating chances by, on average, 0.17%\n",
    "\n",
    "Let's show the residual plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "colab_type": "code",
    "id": "sn8crVSAlPIw",
    "outputId": "d81c4196-3cf4-476d-c403-7cdeabdf975d"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(admit_predict, residuals, color='black')\n",
    "plt.ylabel('residual')\n",
    "plt.xlabel('fitted values')\n",
    "plt.axhline(y= residuals.mean(), color='red', linewidth=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oIT6DpvlmqaD"
   },
   "source": [
    "This does not look too bad: our residuals are centered around a mean that is very close to 0, and there are no glaringly obvious patterns. Let's be thorough though, and perform a heteroskedasticity test.\n",
    "\n",
    "For this we will use [bartlett's test](https://www.itl.nist.gov/div898/handbook/eda/section3/eda357.htm). The test establishes as a null hypothesis that the variance is equal for all our datapoints,and the new hypothesis that the variance is different for at least one pair of datapoints.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "LlvHQ6Ivm5j5",
    "outputId": "ac5d7635-1ab7-43d9-a0ae-093fdb218019"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.918977604620448\n",
      "the variances are homogeneous!\n"
     ]
    }
   ],
   "source": [
    "import scipy as sp\n",
    "\n",
    "test_result, p_value = sp.stats.bartlett(admit_predict, residuals)\n",
    "\n",
    "# To interpret the results we must also compute a critical value of the chi squared distribution\n",
    "degree_of_freedom = len(admit_predict)-1\n",
    "probability = 1 - p_value\n",
    "\n",
    "critical_value = sp.stats.chi2.ppf(probability, degree_of_freedom)\n",
    "print(value)\n",
    "\n",
    "# If the test_result is greater than the critical value, then we reject our null\n",
    "# hypothesis. This would mean that there are patterns to the variance of the data\n",
    "\n",
    "# Otherwise, we can identify no patterns, and we accept the null hypothesis that \n",
    "# the variance is homogeneous across our data\n",
    "\n",
    "if (test_result > critical_value):\n",
    "  print('the variances are unequal, and the model should be reassessed')\n",
    "else:\n",
    "  print('the variances are homogeneous!')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W27Z9P7Jx1yK"
   },
   "source": [
    "# Challenge\n",
    "\n",
    "We will bring this all together by adapting a challenge from data.world. In the code section below you have access to a large dataset with demographic and medical data for cancer occurances in regions in the US. Your aim is to build a model that predicts the **target_deathrate** variable. You can find descriptions of all the columns [here](https://data.world/exercises/linear-regression-exercise-1)\n",
    "\n",
    "This is a holistic challenge:\n",
    "\n",
    "\n",
    "*   Make sure to clean up your data first, there are some missing elements.\n",
    "*   Some data should be changed: Add a **state** column which indicates the state a person lives in(you can modify the geography column). Perform a multicollinearity test: Should **state** be included in your model or not?\n",
    "* If it should be, make sure to transform it since we want to be dealing with numerical data.\n",
    "* Build a model, then display the residual plot for it. Perform a Bartlett test to determine if your model is acceptable or not.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jPivvZE_zCzY"
   },
   "outputs": [],
   "source": [
    "# Your code goes here!\n",
    "df = pd.read_csv('https://query.data.world/s/4n3rdrpeu6qrfo63yacah7v3ofoaag')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Linear regression tips.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
